# -*- coding: utf-8 -*-
"""loan-prediction-model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EL84CSTMcQSK4ez91kiycVhgcax3Arj2

# Loan Default Prediction Model

## Introduction
This notebook presents a comprehensive approach to building a machine learning model for predicting loan default. The goal is to accurately predict whether a loan applicant is likely to default on their loan, using various features related to the applicant's financial and demographic information. The model developed in this notebook is trained on a dataset containing information about loan applications, and the final predictions are evaluated using various performance metrics.

## Dataset Overview
The dataset contains records of loan applicants along with various features such as income, loan amount, credit history, and more. The target variable, `Loan_Status`, indicates whether the loan was approved (`Y`) or not (`N`).

**Features:**
- **ApplicantIncome:** The income of the applicant.
- **CoapplicantIncome:** The income of the co-applicant.
- **LoanAmount:** The loan amount requested.
- **Loan_Amount_Term:** The term of the loan in months.
- **Credit_History:** The credit history of the applicant (1 = good, 0 = bad).
- **Gender, Married, Dependents, Education, Self_Employed, Property_Area:** Various categorical features describing the applicant's demographics.

**Target:**
- **Loan_Status:** Whether the loan was approved (`Y`) or not (`N`).

---

## Table of Contents
1. [Data Preprocessing](#data-preprocessing)
2. [Exploratory Data Analysis (EDA)](#exploratory-data-analysis)
3. [Modeling](#modeling)
   - 3.1 [Baseline Model](#baseline-model)
   - 3.2 [Hyperparameter Tuning](#hyperparameter-tuning)
   - 3.3 [Model Evaluation](#model-evaluation)
4. [Final Model and Predictions](#final-model-and-predictions)
5. [Conclusion](#conclusion)

---
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

import os
os.listdir()

import zipfile


with zipfile.ZipFile('/content/archive (6).zip', 'r') as zip_ref:
    zip_ref.extractall()

os.listdir()

# Load the training dataset
train_df = pd.read_csv('train.csv')

# Load the test dataset
test_df = pd.read_csv('test.csv')

# Display the first few rows of the training dataset
train_df.head()

# Display the first few rows of the test dataset
test_df.head()

"""## Data Preprocessing <a id="data-preprocessing"></a>

In this section, we handle missing data, encode categorical variables, and prepare the dataset for modeling. Proper data preprocessing is crucial to ensure that the machine learning algorithms can effectively learn from the data.

### 2.1 Handling Missing Values
We start by identifying and addressing any missing values in the dataset. Missing data can lead to biased models if not handled properly.

### 2.2 Encoding Categorical Variables
Next, we convert categorical variables into a numerical format using one-hot encoding. This step is essential because most machine learning models require numerical input.

### 2.3 Feature Scaling (Optional)
Depending on the model used, we might scale the features to ensure that they are on a similar scale. This can help improve the model's performance.

---



"""

# Summary of the training dataset
train_df.info()

# Check for missing values in the training data
train_df.isnull().sum()

# Summary of the test dataset
test_df.info()

# Check for missing values in the test data
test_df.isnull().sum()

# Fill missing values for categorical features with the mode
train_df['Gender'].fillna(train_df['Gender'].mode()[0], inplace=True)
train_df['Married'].fillna(train_df['Married'].mode()[0], inplace=True)
train_df['Dependents'].fillna(train_df['Dependents'].mode()[0], inplace=True)
train_df['Self_Employed'].fillna(train_df['Self_Employed'].mode()[0], inplace=True)

# Fill missing values for numerical features with the median
train_df['LoanAmount'].fillna(train_df['LoanAmount'].median(), inplace=True)
train_df['Loan_Amount_Term'].fillna(train_df['Loan_Amount_Term'].median(), inplace=True)
train_df['Credit_History'].fillna(train_df['Credit_History'].mode()[0], inplace=True)

# Fill missing values for categorical features with the mode
test_df['Gender'].fillna(test_df['Gender'].mode()[0], inplace=True)
test_df['Dependents'].fillna(test_df['Dependents'].mode()[0], inplace=True)
test_df['Self_Employed'].fillna(test_df['Self_Employed'].mode()[0], inplace=True)

# Fill missing values for numerical features with the median
test_df['LoanAmount'].fillna(test_df['LoanAmount'].median(), inplace=True)
test_df['Loan_Amount_Term'].fillna(test_df['Loan_Amount_Term'].median(), inplace=True)
test_df['Credit_History'].fillna(test_df['Credit_History'].mode()[0], inplace=True)

# Encode categorical variables in the training data
train_df = pd.get_dummies(train_df, drop_first=True)

# Encode categorical variables in the test data
test_df = pd.get_dummies(test_df, drop_first=True)

# Display the columns in the DataFrame after encoding
print(train_df.columns)

# Check if 'Loan_Status' is in the columns of the DataFrame
print('Loan_Status' in train_df.columns)

# Display the columns in the DataFrame to confirm
print(train_df.columns)

# Load the original training dataset again
original_train_df = pd.read_csv('train.csv')

# Check if 'Loan_Status' is present in the original data
print('Loan_Status' in original_train_df.columns)

# Display the columns in the original DataFrame to confirm
print(original_train_df.columns)

# Reload the original training dataset
original_train_df = pd.read_csv('train.csv')

# Ensure 'Loan_Status' is properly encoded as 1 for 'Y' and 0 for 'N'
original_train_df['Loan_Status'] = original_train_df['Loan_Status'].map({'Y': 1, 'N': 0})

# Merge the 'Loan_Status' column back into the processed train_df DataFrame
train_df['Loan_Status'] = original_train_df['Loan_Status']

# Verify the merge by checking the first few rows
print(train_df.head())

# Separate features and target variable
X = train_df.drop('Loan_Status', axis=1)
y = train_df['Loan_Status']

# Display the first few rows of features and target variable to verify
print(X.head())
print(y.head())

from sklearn.model_selection import train_test_split

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)

# Check the shapes to confirm the split
print(f"X_train shape: {X_train.shape}")
print(f"X_val shape: {X_val.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_val shape: {y_val.shape}")

from sklearn.ensemble import RandomForestClassifier

# Initialize and train the RandomForest model
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Predict on the validation set
y_val_pred = model.predict(X_val)

# Calculate accuracy
accuracy = accuracy_score(y_val, y_val_pred)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')

# Generate confusion matrix
conf_matrix = confusion_matrix(y_val, y_val_pred)
print('Confusion Matrix:\n', conf_matrix)

# Generate classification report
class_report = classification_report(y_val, y_val_pred)
print('Classification Report:\n', class_report)

# Reload the original test dataset
test_df = pd.read_csv('test.csv')

# Preserve the Loan_ID column separately
loan_ids = test_df['Loan_ID']

# Drop the Loan_ID column from the test_df to avoid issues during preprocessing
test_df = test_df.drop(columns=['Loan_ID'])

# Display the first few rows to verify
test_df.head()

# Fill missing values for numerical features with the median
numeric_features = test_df.select_dtypes(include=[np.number]).columns
test_df[numeric_features] = test_df[numeric_features].fillna(test_df[numeric_features].median())

# Fill missing values for categorical features with the mode
categorical_features = test_df.select_dtypes(include=['object']).columns
test_df[categorical_features] = test_df[categorical_features].apply(lambda x: x.fillna(x.mode()[0]))

# Apply one-hot encoding to categorical variables
test_df = pd.get_dummies(test_df, drop_first=True)

# Align the test_df with train_df's features
test_df = test_df.reindex(columns=X.columns, fill_value=0)

# Display the first few rows to verify
test_df.head()

# Predict on the processed test set
test_predictions = model.predict(test_df)

# Prepare the submission DataFrame
submission = pd.DataFrame({'Loan_ID': loan_ids, 'Loan_Status': test_predictions})

# Convert predictions back to original labels ('Y' for 1 and 'N' for 0)
submission['Loan_Status'] = submission['Loan_Status'].map({1: 'Y', 0: 'N'})

# Save the submission file
submission.to_csv('loan_prediction_submission.csv', index=False)

# Display the first few rows of the submission file to verify
submission.head()

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Predict on the validation set
y_val_pred = model.predict(X_val)

# Calculate accuracy
accuracy = accuracy_score(y_val, y_val_pred)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')

# Generate confusion matrix
conf_matrix = confusion_matrix(y_val, y_val_pred)
print('Confusion Matrix:\n', conf_matrix)

# Generate classification report
class_report = classification_report(y_val, y_val_pred)
print('Classification Report:\n', class_report)

from sklearn.model_selection import cross_val_score

# Perform cross-validation
cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')

# Display cross-validation scores
print(f"Cross-Validation Scores: {cv_scores}")
print(f"Mean CV Accuracy: {cv_scores.mean() * 100:.2f}%")

from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Initialize Grid Search
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')

# Fit Grid Search
grid_search.fit(X_train, y_train)

# Best parameters
print(f"Best Parameters: {grid_search.best_params_}")

# Best score
print(f"Best Cross-Validation Score: {grid_search.best_score_ * 100:.2f}%")

# Re-train the model on the entire training set
final_model = RandomForestClassifier(random_state=42, **grid_search.best_params_)
final_model.fit(X, y)

# Make final predictions on the test set
final_test_predictions = final_model.predict(test_df)

# Prepare the final submission DataFrame
final_submission = pd.DataFrame({'Loan_ID': loan_ids, 'Loan_Status': final_test_predictions})

# Convert predictions to 'Y'/'N'
final_submission['Loan_Status'] = final_submission['Loan_Status'].map({1: 'Y', 0: 'N'})

# Save the final submission file
final_submission.to_csv('final_loan_prediction_submission.csv', index=False)

# Retrain the RandomForest model using the best parameters
best_params = {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}
final_model = RandomForestClassifier(random_state=42, **best_params)
final_model.fit(X, y)

# Predict on the validation set to see how the model performs with the best parameters
y_val_pred = final_model.predict(X_val)

# Calculate accuracy
accuracy = accuracy_score(y_val, y_val_pred)
print(f'Validation Accuracy with Best Params: {accuracy * 100:.2f}%')

# Generate confusion matrix
conf_matrix = confusion_matrix(y_val, y_val_pred)
print('Confusion Matrix with Best Params:\n', conf_matrix)

# Generate classification report
class_report = classification_report(y_val, y_val_pred)
print('Classification Report with Best Params:\n', class_report)

# Predict on the test set
final_test_predictions = final_model.predict(test_df)

# Prepare the final submission DataFrame
final_submission = pd.DataFrame({'Loan_ID': loan_ids, 'Loan_Status': final_test_predictions})

# Convert predictions back to original labels ('Y' for 1 and 'N' for 0)
final_submission['Loan_Status'] = final_submission['Loan_Status'].map({1: 'Y', 0: 'N'})

# Save the final submission file
final_submission.to_csv('final_loan_prediction_submission.csv', index=False)

# Display the first few rows of the submission file to verify
print(final_submission.head())

